{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple avec des modèles N-grammes de mots\n",
    "## Survol des principales fonctionnalités de NLTK.\n",
    "## Découpage des textes en sous-séquences de longueur N, c.-à-d. en N-grammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Le', 'cours'), ('cours', 'IFT-7022'), ('IFT-7022', 'est'), ('est', 'offert'), ('offert', 'à'), ('à', 'distance'), ('distance', 'cette'), ('cette', 'année'), ('année', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "\n",
    "text = \"Le cours IFT-7022 est offert à distance cette année.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(list(bigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Le', 'cours', 'IFT-7022'), ('cours', 'IFT-7022', 'est'), ('IFT-7022', 'est', 'offert'), ('est', 'offert', 'à'), ('offert', 'à', 'distance'), ('à', 'distance', 'cette'), ('distance', 'cette', 'année'), ('cette', 'année', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(trigrams(tokens))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de symboles de début et de fin de séquence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>',\n",
       " 'Le',\n",
       " 'cours',\n",
       " 'IFT-7022',\n",
       " 'est',\n",
       " 'offert',\n",
       " 'à',\n",
       " 'distance',\n",
       " 'cette',\n",
       " 'année',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence \n",
    "\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "text = \"Le cours IFT-7022 est offert à distance cette année.\"\n",
    "tokens = word_tokenize(text)\n",
    "list(pad_sequence(tokens, pad_left=True, left_pad_symbol=BOS, pad_right=True, right_pad_symbol=EOS, n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les étapes suivantes donnnent un exemple simple de construction de modèle N-gramme.\n",
    "Les étapes sont:\n",
    "\n",
    "* la construction du vocabulaire\n",
    "* la construction des n-grammes\n",
    "* l'entraînement du modèle n-gramme\n",
    "* l'utilisation du modèle pour estimer des probabilités ou des perplexités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', '.', 'à', 'ift', 'cours', 'habituellement', 'automne', 'ce', 'offert', 'cette', 'année', 'pas', 'donnée', 'le', 'l', '7022', 'est', 'n', 'distance']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def build_vocabulary(text_list):\n",
    "    all_unigrams = list()\n",
    "    for sentence in text_list:\n",
    "        word_list = word_tokenize(sentence.lower())\n",
    "        all_unigrams = all_unigrams + word_list\n",
    "    voc = set(all_unigrams)\n",
    "    voc.add(BOS)\n",
    "    return list(voc)\n",
    "\n",
    "corpus = [\"le cours ift 7022 est offert à distance cette année .\", \n",
    "          \"ce cours n est habituellement pas à distance .\",\n",
    "          \"le cours est habituellement donnée à l automne .\"]\n",
    "\n",
    "vocabulary = build_vocabulary(corpus)\n",
    "print(vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'le'),\n",
       " ('le', 'cours'),\n",
       " ('cours', 'ift'),\n",
       " ('ift', '7022'),\n",
       " ('7022', 'est'),\n",
       " ('est', 'offert'),\n",
       " ('offert', 'à'),\n",
       " ('à', 'distance'),\n",
       " ('distance', 'cette'),\n",
       " ('cette', 'année'),\n",
       " ('année', '.'),\n",
       " ('.', '<EOS>'),\n",
       " ('<BOS>', 'ce'),\n",
       " ('ce', 'cours'),\n",
       " ('cours', 'n'),\n",
       " ('n', 'est'),\n",
       " ('est', 'habituellement'),\n",
       " ('habituellement', 'pas'),\n",
       " ('pas', 'à'),\n",
       " ('à', 'distance'),\n",
       " ('distance', '.'),\n",
       " ('.', '<EOS>'),\n",
       " ('<BOS>', 'le'),\n",
       " ('le', 'cours'),\n",
       " ('cours', 'est'),\n",
       " ('est', 'habituellement'),\n",
       " ('habituellement', 'donnée'),\n",
       " ('donnée', 'à'),\n",
       " ('à', 'l'),\n",
       " ('l', 'automne'),\n",
       " ('automne', '.'),\n",
       " ('.', '<EOS>')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text_list, n=2):\n",
    "    all_ngrams = list()\n",
    "    for sentence in text_list:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        padded_sent = list(pad_sequence(tokens, pad_left=True, left_pad_symbol=BOS, pad_right=True, right_pad_symbol=EOS, n=n))\n",
    "        all_ngrams = all_ngrams + list(ngrams(padded_sent, n=n))      \n",
    "    return all_ngrams\n",
    "\n",
    "order = 2\n",
    "\n",
    "corpus_ngrams = get_ngrams(corpus, n=order)\n",
    "display(corpus_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "model = MLE(order)\n",
    "model.fit([corpus_ngrams], vocabulary_text=vocabulary)\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"ift\", [\"cours\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5849625007211563"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logscore(\"ift\", [\"cours\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688774"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequence = [(\"le\", \"cours\"), (\"cours\", \"ift\")]\n",
    "model.perplexity(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'est'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(text_seed=['cours'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lissage de probabilités\n",
    "Il faut prévoir que le modèle n'aura pas tout vu à l'entraînement. Et qu'il ne sera pas en mesure d'évaluer de nouvelles séquences de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité de \"ce soir\" =  0.0\n",
      "Perplexité de la séquence de test =  inf\n"
     ]
    }
   ],
   "source": [
    "test_sequence = [(\"le\", \"cours\"), (\"cours\", \"ift\"), (\"ift\", \"est\"), (\"est\", \"ce\"), (\"ce\", \"soir\"), (\"soir\", \".\")]\n",
    "print(\"Probabilité de \\\"ce soir\\\" = \", model.score(\"soir\", [\"ce\"]))\n",
    "print(\"Perplexité de la séquence de test = \", model.perplexity(test_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit alors attribuer des probabilités aux n-grammes inconnus à l'aide d'une technique de lissage. Ici, on applique un lissage de Laplace pour corriger notre problème. D'autres types de lissage sont disponibles dans NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité de \"ce soir\" =  0.047619047619047616\n",
      "Perplexité de la séquence de test =  16.052128016712338\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.models import Laplace\n",
    "\n",
    "model = Laplace(order)\n",
    "model.fit([corpus_ngrams], vocabulary_text=vocabulary)\n",
    "print(\"Probabilité de \\\"ce soir\\\" = \", model.score(\"soir\", [\"ce\"]))\n",
    "print(\"Perplexité de la séquence de test = \", model.perplexity(test_sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
