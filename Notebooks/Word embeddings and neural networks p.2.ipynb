{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks # 2: Document Classification with a Multilayer Network\n",
    "This example of a multilayer network (multilayer perceptron - MLP) is presented. This example is almost identical to the one on single layer perceptrons. The main differences are in the definition of the network architecture (part 2) and the parameters which are passed to the Experiment class to train the model.\n",
    "\n",
    "1. Creation of the dataset\n",
    "\n",
    "As in the previous example, we create our **training and validation sets** with the 20newsgroup corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# On utilise le corpus 20Newsgroups et on limite les exemples d'entraînement à 4 classes\n",
    "wanted_categories = ['rec.sport.hockey', 'sci.space', 'rec.autos', 'sci.med']\n",
    "training_corpus = fetch_20newsgroups(subset='train', categories=wanted_categories, shuffle=True)\n",
    "validation_corpus = fetch_20newsgroups(subset='test', categories=wanted_categories, shuffle=True)\n",
    "target_categories = training_corpus.target_names\n",
    "\n",
    "# On créer un Bag-of-Words avec l'ensemble d'entrainement\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "X_train = vectorizer.fit_transform(training_corpus.data)\n",
    "y_train = training_corpus.target\n",
    "\n",
    "# On réutilise la transformation sur l'ensemble de validation\n",
    "X_valid = vectorizer.transform(validation_corpus.data)\n",
    "y_valid = validation_corpus.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation of a multilayer neural network architecture\n",
    "The network architecture of this example contains 2 layers:\n",
    "\n",
    "* a first which converts a document vector into an intermediate representation (the hidden layer) and\n",
    "* another that produces the output values from the hidden layer.\n",
    "\n",
    "The 2 layers correspond to linear transformations (z = Wx + b).\n",
    "\n",
    "And we apply an activation function of the **RELU** (Rectified Linear Unit) type to the output of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "bow_size = X_train.shape[1]\n",
    "nb_classes = len(target_categories)\n",
    "\n",
    "# input : un vecteur de mots\n",
    "# output: les différentes classes de notre problème de classification\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_layer_size, output_size) :\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = nn.functional.relu_(x)\n",
    "        x = self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creation of a dataloader to iterate on the data in minibatch\n",
    "This part is identical to that of the previous example on perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "class SparseMatrixDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_in_csr_matrix: csr_matrix, target: np.array):\n",
    "        self.dataset = dataset_in_csr_matrix\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return FloatTensor(self.dataset[index,:].todense()).squeeze(0), LongTensor([self.target[index]]).squeeze(0)\n",
    "        \n",
    "def get_dataloader(base_dataset, dataset_target, dataset_class):\n",
    "    return DataLoader(dataset_class(base_dataset, dataset_target), batch_size=16, shuffle=True)\n",
    "\n",
    "train_loader = get_dataloader(X_train, y_train, SparseMatrixDataset)\n",
    "valid_loader = get_dataloader(X_valid, y_valid, SparseMatrixDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a training loop\n",
    "This part is also similar to the previous example.\n",
    "\n",
    "The main difference with the previous example is that we must define the size of the hidden layer which has 100 neurons (hidden_size = 100). This choice is arbitrary and could be determined by a grid search type exploration.\n",
    "\n",
    "Another more minor difference: Experiment is told to save the model and the training statistics in the 'model / mlp' directory.\n",
    "\n",
    "Note: Patience is essential when working with neural networks. Despite the small size of the network, training the network takes a few minutes (about 7 seconds per epoch on my computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (input_layer): Linear(in_features=37000, out_features=100, bias=True)\n",
      "  (output_layer): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100\n",
    "\n",
    "model = MultiLayerPerceptron(bow_size, hidden_size, nb_classes)\n",
    "print(model)\n",
    "\n",
    "experiment = Experiment('model/mlp', model, optimizer = \"SGD\", task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = experiment.train(train_loader, valid_loader, epochs=30, disable_tensorboard=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions with the model\n",
    "Now that the model is trained, we test it on new examples to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax \n",
    "\n",
    "def get_most_probable_class(sentence, model):\n",
    "    vectorized_sentence = vectorizer.transform([sentence]).todense()\n",
    "    prediction = model(FloatTensor(vectorized_sentence).squeeze(0)).detach()\n",
    "    output = softmax(prediction, dim=0)\n",
    "    max_category_index = np.argmax(output)\n",
    "    max_category = target_categories[max_category_index]\n",
    "    print(\"\\nClassification de la phrase: \", sentence)\n",
    "    print(\"Sorties du réseau de neurones:\", prediction)\n",
    "    print(\"Valeurs obtenues après application de softmax:\", output)\n",
    "    print(\"Meilleure classe: {} qui correspond en sortie au neurone {}\".format(max_category, max_category_index))\n",
    "    return(max_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = ['Getzky was a center, not a goaltender', \n",
    "             'Mazda and BMW cars are esthetic',\n",
    "             'Doctor, doctor, gimme the news', \n",
    "             'Take me to the moon']\n",
    "\n",
    "[get_most_probable_class(sentence, model) for sentence in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test le modèle avec de plus longues phrases, ce qui semble régler notre problème\n",
    "\n",
    "test_docs = ['Getzky was a center, not a goaltender but a fantastic hockey player', \n",
    "             'Mazda and BMW are esthetic cars but the motors are quite different',\n",
    "             'Doctor, doctor, gimme the news', \n",
    "             'Take me to the moon, the sun and planet Mars']\n",
    "\n",
    "[get_most_probable_class(sentence, model) for sentence in test_docs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
