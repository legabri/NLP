{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification de texte avec embeddings et réseau récurrent\n",
    "\n",
    "Ce notebook présente un exemple de classification de texte avec un réseau récurrent. On utilise l'exemple de la classification de question, vu à la 3e semaine sur la classification de texte. Par exemple, la question Where is John Wayne airport ? est de type LOCATION.\n",
    "\n",
    "Les principales composantes de ce réseau sont:\n",
    "\n",
    "* des plongements de mots provenant de Spacy\n",
    "* l'encodage des questions avec un réseau LSTM bidirectionnel (voir notes de cours).\n",
    "* une couche linéaire en sortie pour faire la classification, c.-à-d. déterminer le type de question.\n",
    "\n",
    "### 1. Création des jeux de données d'entraînement et de validation\n",
    "\n",
    "On monte ici les données d'entraînement (disponible sur le site du cours dans la section \"classification de texte). Dans cet exemple, les données sont partionnées 80%-20%, ce dernier ensemble étant utilisé pour déterminer l'époque (epoch) qui obtient le meilleur modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"./data_rnn/questions-t3.txt\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        labels, questions = zip(*[tuple(s.split(' ', 1)) for s in lines])\n",
    "    return questions, labels\n",
    "\n",
    "questions, labels = load_dataset(train_dataset_path)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(questions, labels, test_size=0.2, shuffle=True,random_state=42)\n",
    "\n",
    "# On converti les label textuels en index numérique\n",
    "id2lable = {label_id:value for label_id, value in enumerate(list(set(labels)))}\n",
    "label2id = {value:label_id for label_id, value in id2lable.items()}\n",
    "\n",
    "y_train = [label2id[label] for label in y_train]\n",
    "y_valid = [label2id[label] for label in y_valid]\n",
    "\n",
    "nb_class = len(id2lable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion du vocabulaire et de la vectorisation des mots\n",
    "\n",
    "En utilisant uniquement les mots contenus dans l'ensemble d'entrainement, on peut construire le vocabulaire de notre corpus. Spacy sera utilisé pour faire la tokénisation des mots ainsi que pour leur attribuer un plongement (embedding - word.vector).\n",
    "\n",
    "Lors du test, on consultera le vocabulaire pour voir si le mot a été vu à l'entraînement. Si un mot n'a pas été vu, on le considère comme un mot inconnu .\n",
    "\n",
    "Pour gérer le vocabulaire et les embeddings de mots, on construit des tables de correspondance permettant de : 1 - obtenir l'index d'un mot (afin de convertir les questions en séquence d'index) 2 - obtenir son embedding à partir de l'index d'un mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "embedding_size = nlp.meta['vectors']['width']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2embedding = {}\n",
    "\n",
    "word2id[1] = \"<unk>\"\n",
    "\n",
    "id2embedding[1] = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "word_index = 2\n",
    "\n",
    "for question in X_train:\n",
    "    for word in nlp(question):\n",
    "        if word.text not in word2id.keys():\n",
    "            word2id[word.text] = word_index\n",
    "            id2embedding[word_index] = word.vector\n",
    "            word_index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée ici la classe TokenisedDataset qui sera utilisée par les dataloader pour gérer les textes durant l'entraînement du modèle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import LongTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class TokenisedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset: List[str] , target: np.array, word2id: Dict[str, int], nlp_model):\n",
    "        self.tokenized_dataset = [None for _ in range(len(dataset))]\n",
    "        self.dataset = dataset\n",
    "        self.target = target\n",
    "        self.word2id = word2id\n",
    "        self.nlp_model = nlp_model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.tokenized_dataset[index] is None:\n",
    "            self.tokenized_dataset[index] = self.tokenize(self.dataset[index])\n",
    "        # print(self.tokenized_dataset[index])\n",
    "        # print([self.target[index]])\n",
    "        \n",
    "        return LongTensor(self.tokenized_dataset[index]), LongTensor([self.target[index]]).squeeze(0)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        return [ self.word2id.get(word.text, 1) for word in self.nlp_model(sentence)]\n",
    "    \n",
    "    \n",
    "train_dataset = TokenisedDataset(X_train, y_train, word2id, nlp)\n",
    "valid_dataset = TokenisedDataset(X_valid, y_valid, word2id, nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construction de l'architecture du réseau\n",
    "L'architecture du réseau récurrent est la suivante:\n",
    "\n",
    "* une couche en entrée qui prend les embeddings de mots de Spacy. La taille de la couche d'entrée correspond à la taille d'embedding de Spacy.\n",
    "* une couche cachée récurrent qui prend en entrée un embedding de mot et l'état caché précédent. Les neurones de cette couche sont de type LSTM, une structure de neurone qui facilite la propagation d'information sur de plus longues séquences. À noter que la couche est bi-directionnelle (voir note de cours).\n",
    "* une couche de classification qui donne en sortie un score pour chacune des classes (types de question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNNWithEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_state_size, nb_class) :\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding)\n",
    "        embedding_size = embedding.size()[1]\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_state_size, 1, bidirectional=True)        \n",
    "        self.classification_layer = nn.Linear(2 * hidden_state_size, nb_class) # Une pour chaque direction\n",
    "    \n",
    "    def forward(self, x, x_lenghts):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self._handle_rnn_output(x, x_lenghts)\n",
    "        x = self.classification_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _handle_rnn_output(self, x, x_lenghts):\n",
    "        # On \"pack\" les batch pour les envoyer dans le RNN\n",
    "        packed_batch = pack_padded_sequence(x, x_lenghts, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # On ne conserve que le hidden state de la dernière cellule\n",
    "        # full output, (last_hidden_state, last_cell_state) = ...\n",
    "        _, (last_hidden_state, _) = self.rnn(packed_batch)\n",
    "        \n",
    "        # On remet la batch comme première dimension\n",
    "        x = torch.transpose(last_hidden_state,0,1)\n",
    "        \n",
    "        # On concatene les vecteurs de chacune des directions du LSTM\n",
    "        x = x.reshape(len(x_lenghts),-1)\n",
    "                \n",
    "            \n",
    "        return x\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch : List[Tuple[LongTensor, LongTensor]]) -> Tuple[LongTensor, LongTensor]:\n",
    "    x = [x for x,y in batch]\n",
    "    x_true_length = [len(x) for x,y in batch]\n",
    "    y = torch.stack([y for x,y in batch], dim=0)\n",
    "    \n",
    "    return ((pad_sequence(x, batch_first=True), x_true_length), y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prochaine section est un artefact mécanique permettant de mettre les vecteurs d'embeddings directement dans l'architecture neuronale (les valeurs des embeddings correspondent aux poids des liens de la couche). On génère la table de correspondance entre les index des mots et les embeddings dans le format attendu par PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2embedding[0] = np.zeros(embedding_size, dtype=np.float32)\n",
    "embedding_layer = np.zeros((len(id2embedding), embedding_size), dtype=np.float32)\n",
    "for token_index, embedding in id2embedding.items():\n",
    "    embedding_layer[token_index,:] = embedding\n",
    "embedding_layer = torch.from_numpy(embedding_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement du modèle\n",
    "Cette partie devrait vous être familière si vous avez étudié les exemples des semaines précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "import numpy as np\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100\n",
    "\n",
    "model = RNNWithEmbeddingLayer(embedding_layer, hidden_size, nb_class)\n",
    "experiment = Experiment('model/embeddings_rnn', \n",
    "                        model, \n",
    "                        optimizer = \"SGD\", \n",
    "                        task=\"classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m1/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m117.37s \u001b[35mloss:\u001b[94m 2.070367\u001b[35m acc:\u001b[94m 21.737174\u001b[35m fscore_micro:\u001b[94m 0.217372\u001b[35m val_loss:\u001b[94m 1.977740\u001b[35m val_acc:\u001b[94m 22.571942\u001b[35m val_fscore_micro:\u001b[94m 0.225719\u001b[0m\n",
      "Epoch 1: val_acc improved from -inf to 22.57194, saving file to model/embeddings_rnn\\checkpoint_epoch_1.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m2/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m24.80s \u001b[35mloss:\u001b[94m 1.960707\u001b[35m acc:\u001b[94m 27.160216\u001b[35m fscore_micro:\u001b[94m 0.271602\u001b[35m val_loss:\u001b[94m 1.914909\u001b[35m val_acc:\u001b[94m 31.564748\u001b[35m val_fscore_micro:\u001b[94m 0.315647\u001b[0m\n",
      "Epoch 2: val_acc improved from 22.57194 to 31.56475, saving file to model/embeddings_rnn\\checkpoint_epoch_2.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m3/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m28.67s \u001b[35mloss:\u001b[94m 1.902836\u001b[35m acc:\u001b[94m 31.840684\u001b[35m fscore_micro:\u001b[94m 0.318407\u001b[35m val_loss:\u001b[94m 1.853014\u001b[35m val_acc:\u001b[94m 32.913669\u001b[35m val_fscore_micro:\u001b[94m 0.329137\u001b[0m\n",
      "Epoch 3: val_acc improved from 31.56475 to 32.91367, saving file to model/embeddings_rnn\\checkpoint_epoch_3.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m4/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m30.70s \u001b[35mloss:\u001b[94m 1.830566\u001b[35m acc:\u001b[94m 34.068407\u001b[35m fscore_micro:\u001b[94m 0.340684\u001b[35m val_loss:\u001b[94m 1.767528\u001b[35m val_acc:\u001b[94m 36.510791\u001b[35m val_fscore_micro:\u001b[94m 0.365108\u001b[0m\n",
      "Epoch 4: val_acc improved from 32.91367 to 36.51079, saving file to model/embeddings_rnn\\checkpoint_epoch_4.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m5/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m35.06s \u001b[35mloss:\u001b[94m 1.736506\u001b[35m acc:\u001b[94m 42.259226\u001b[35m fscore_micro:\u001b[94m 0.422592\u001b[35m val_loss:\u001b[94m 1.665011\u001b[35m val_acc:\u001b[94m 45.773381\u001b[35m val_fscore_micro:\u001b[94m 0.457734\u001b[0m\n",
      "Epoch 5: val_acc improved from 36.51079 to 45.77338, saving file to model/embeddings_rnn\\checkpoint_epoch_5.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m6/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m30.33s \u001b[35mloss:\u001b[94m 1.629029\u001b[35m acc:\u001b[94m 51.957696\u001b[35m fscore_micro:\u001b[94m 0.519577\u001b[35m val_loss:\u001b[94m 1.553847\u001b[35m val_acc:\u001b[94m 56.834532\u001b[35m val_fscore_micro:\u001b[94m 0.568345\u001b[0m\n",
      "Epoch 6: val_acc improved from 45.77338 to 56.83453, saving file to model/embeddings_rnn\\checkpoint_epoch_6.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m7/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m28.73s \u001b[35mloss:\u001b[94m 1.514692\u001b[35m acc:\u001b[94m 57.110711\u001b[35m fscore_micro:\u001b[94m 0.571107\u001b[35m val_loss:\u001b[94m 1.441000\u001b[35m val_acc:\u001b[94m 60.791367\u001b[35m val_fscore_micro:\u001b[94m 0.607914\u001b[0m\n",
      "Epoch 7: val_acc improved from 56.83453 to 60.79137, saving file to model/embeddings_rnn\\checkpoint_epoch_7.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m8/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m29.93s \u001b[35mloss:\u001b[94m 1.398632\u001b[35m acc:\u001b[94m 59.113411\u001b[35m fscore_micro:\u001b[94m 0.591134\u001b[35m val_loss:\u001b[94m 1.327186\u001b[35m val_acc:\u001b[94m 60.251799\u001b[35m val_fscore_micro:\u001b[94m 0.602518\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m9/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m32.00s \u001b[35mloss:\u001b[94m 1.283117\u001b[35m acc:\u001b[94m 59.855986\u001b[35m fscore_micro:\u001b[94m 0.598560\u001b[35m val_loss:\u001b[94m 1.214336\u001b[35m val_acc:\u001b[94m 61.690647\u001b[35m val_fscore_micro:\u001b[94m 0.616906\u001b[0m\n",
      "Epoch 9: val_acc improved from 60.79137 to 61.69065, saving file to model/embeddings_rnn\\checkpoint_epoch_9.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m10/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m30.21s \u001b[35mloss:\u001b[94m 1.180522\u001b[35m acc:\u001b[94m 63.028803\u001b[35m fscore_micro:\u001b[94m 0.630288\u001b[35m val_loss:\u001b[94m 1.130287\u001b[35m val_acc:\u001b[94m 66.456835\u001b[35m val_fscore_micro:\u001b[94m 0.664568\u001b[0m\n",
      "Epoch 10: val_acc improved from 61.69065 to 66.45683, saving file to model/embeddings_rnn\\checkpoint_epoch_10.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m11/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m29.86s \u001b[35mloss:\u001b[94m 1.091022\u001b[35m acc:\u001b[94m 66.291629\u001b[35m fscore_micro:\u001b[94m 0.662916\u001b[35m val_loss:\u001b[94m 1.056564\u001b[35m val_acc:\u001b[94m 68.435252\u001b[35m val_fscore_micro:\u001b[94m 0.684353\u001b[0m\n",
      "Epoch 11: val_acc improved from 66.45683 to 68.43525, saving file to model/embeddings_rnn\\checkpoint_epoch_11.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m12/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m32.36s \u001b[35mloss:\u001b[94m 1.012302\u001b[35m acc:\u001b[94m 69.126913\u001b[35m fscore_micro:\u001b[94m 0.691269\u001b[35m val_loss:\u001b[94m 0.981014\u001b[35m val_acc:\u001b[94m 70.593525\u001b[35m val_fscore_micro:\u001b[94m 0.705935\u001b[0m\n",
      "Epoch 12: val_acc improved from 68.43525 to 70.59353, saving file to model/embeddings_rnn\\checkpoint_epoch_12.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m13/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m34.00s \u001b[35mloss:\u001b[94m 0.941661\u001b[35m acc:\u001b[94m 70.927093\u001b[35m fscore_micro:\u001b[94m 0.709271\u001b[35m val_loss:\u001b[94m 0.911329\u001b[35m val_acc:\u001b[94m 72.571942\u001b[35m val_fscore_micro:\u001b[94m 0.725719\u001b[0m\n",
      "Epoch 13: val_acc improved from 70.59353 to 72.57194, saving file to model/embeddings_rnn\\checkpoint_epoch_13.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m14/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m35.20s \u001b[35mloss:\u001b[94m 0.880017\u001b[35m acc:\u001b[94m 72.929793\u001b[35m fscore_micro:\u001b[94m 0.729298\u001b[35m val_loss:\u001b[94m 0.860983\u001b[35m val_acc:\u001b[94m 73.830935\u001b[35m val_fscore_micro:\u001b[94m 0.738309\u001b[0m\n",
      "Epoch 14: val_acc improved from 72.57194 to 73.83094, saving file to model/embeddings_rnn\\checkpoint_epoch_14.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m15/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m26.54s \u001b[35mloss:\u001b[94m 0.827775\u001b[35m acc:\u001b[94m 74.819982\u001b[35m fscore_micro:\u001b[94m 0.748200\u001b[35m val_loss:\u001b[94m 0.816916\u001b[35m val_acc:\u001b[94m 74.640288\u001b[35m val_fscore_micro:\u001b[94m 0.746403\u001b[0m\n",
      "Epoch 15: val_acc improved from 73.83094 to 74.64029, saving file to model/embeddings_rnn\\checkpoint_epoch_15.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m16/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m33.47s \u001b[35mloss:\u001b[94m 0.782583\u001b[35m acc:\u001b[94m 75.810081\u001b[35m fscore_micro:\u001b[94m 0.758101\u001b[35m val_loss:\u001b[94m 0.805339\u001b[35m val_acc:\u001b[94m 74.010791\u001b[35m val_fscore_micro:\u001b[94m 0.740108\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m17/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m27.02s \u001b[35mloss:\u001b[94m 0.743751\u001b[35m acc:\u001b[94m 76.935194\u001b[35m fscore_micro:\u001b[94m 0.769352\u001b[35m val_loss:\u001b[94m 0.744915\u001b[35m val_acc:\u001b[94m 76.708633\u001b[35m val_fscore_micro:\u001b[94m 0.767086\u001b[0m\n",
      "Epoch 17: val_acc improved from 74.64029 to 76.70863, saving file to model/embeddings_rnn\\checkpoint_epoch_17.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m18/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m33.62s \u001b[35mloss:\u001b[94m 0.701761\u001b[35m acc:\u001b[94m 78.600360\u001b[35m fscore_micro:\u001b[94m 0.786004\u001b[35m val_loss:\u001b[94m 0.732807\u001b[35m val_acc:\u001b[94m 76.798561\u001b[35m val_fscore_micro:\u001b[94m 0.767986\u001b[0m\n",
      "Epoch 18: val_acc improved from 76.70863 to 76.79856, saving file to model/embeddings_rnn\\checkpoint_epoch_18.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m19/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m21.71s \u001b[35mloss:\u001b[94m 0.667139\u001b[35m acc:\u001b[94m 79.320432\u001b[35m fscore_micro:\u001b[94m 0.793204\u001b[35m val_loss:\u001b[94m 0.684292\u001b[35m val_acc:\u001b[94m 78.417266\u001b[35m val_fscore_micro:\u001b[94m 0.784173\u001b[0m\n",
      "Epoch 19: val_acc improved from 76.79856 to 78.41727, saving file to model/embeddings_rnn\\checkpoint_epoch_19.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m20/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m22.89s \u001b[35mloss:\u001b[94m 0.636343\u001b[35m acc:\u001b[94m 80.355536\u001b[35m fscore_micro:\u001b[94m 0.803555\u001b[35m val_loss:\u001b[94m 0.661626\u001b[35m val_acc:\u001b[94m 78.956835\u001b[35m val_fscore_micro:\u001b[94m 0.789568\u001b[0m\n",
      "Epoch 20: val_acc improved from 78.41727 to 78.95683, saving file to model/embeddings_rnn\\checkpoint_epoch_20.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m21/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m26.92s \u001b[35mloss:\u001b[94m 0.611438\u001b[35m acc:\u001b[94m 80.850585\u001b[35m fscore_micro:\u001b[94m 0.808506\u001b[35m val_loss:\u001b[94m 0.666340\u001b[35m val_acc:\u001b[94m 78.327338\u001b[35m val_fscore_micro:\u001b[94m 0.783273\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m22/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.91s \u001b[35mloss:\u001b[94m 0.583319\u001b[35m acc:\u001b[94m 81.413141\u001b[35m fscore_micro:\u001b[94m 0.814131\u001b[35m val_loss:\u001b[94m 0.627289\u001b[35m val_acc:\u001b[94m 80.215827\u001b[35m val_fscore_micro:\u001b[94m 0.802158\u001b[0m\n",
      "Epoch 22: val_acc improved from 78.95683 to 80.21583, saving file to model/embeddings_rnn\\checkpoint_epoch_22.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m23/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.70s \u001b[35mloss:\u001b[94m 0.560419\u001b[35m acc:\u001b[94m 82.403240\u001b[35m fscore_micro:\u001b[94m 0.824032\u001b[35m val_loss:\u001b[94m 0.615640\u001b[35m val_acc:\u001b[94m 80.665468\u001b[35m val_fscore_micro:\u001b[94m 0.806655\u001b[0m\n",
      "Epoch 23: val_acc improved from 80.21583 to 80.66547, saving file to model/embeddings_rnn\\checkpoint_epoch_23.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m24/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m18.67s \u001b[35mloss:\u001b[94m 0.531619\u001b[35m acc:\u001b[94m 83.303330\u001b[35m fscore_micro:\u001b[94m 0.833033\u001b[35m val_loss:\u001b[94m 0.593748\u001b[35m val_acc:\u001b[94m 81.294964\u001b[35m val_fscore_micro:\u001b[94m 0.812950\u001b[0m\n",
      "Epoch 24: val_acc improved from 80.66547 to 81.29496, saving file to model/embeddings_rnn\\checkpoint_epoch_24.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m25/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m19.51s \u001b[35mloss:\u001b[94m 0.511028\u001b[35m acc:\u001b[94m 83.595860\u001b[35m fscore_micro:\u001b[94m 0.835959\u001b[35m val_loss:\u001b[94m 0.605064\u001b[35m val_acc:\u001b[94m 80.935252\u001b[35m val_fscore_micro:\u001b[94m 0.809352\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m26/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.14s \u001b[35mloss:\u001b[94m 0.492644\u001b[35m acc:\u001b[94m 84.810981\u001b[35m fscore_micro:\u001b[94m 0.848110\u001b[35m val_loss:\u001b[94m 0.654591\u001b[35m val_acc:\u001b[94m 78.597122\u001b[35m val_fscore_micro:\u001b[94m 0.785971\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m27/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.07s \u001b[35mloss:\u001b[94m 0.465031\u001b[35m acc:\u001b[94m 85.373537\u001b[35m fscore_micro:\u001b[94m 0.853735\u001b[35m val_loss:\u001b[94m 0.600572\u001b[35m val_acc:\u001b[94m 79.496403\u001b[35m val_fscore_micro:\u001b[94m 0.794964\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m28/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.41s \u001b[35mloss:\u001b[94m 0.450561\u001b[35m acc:\u001b[94m 85.756076\u001b[35m fscore_micro:\u001b[94m 0.857561\u001b[35m val_loss:\u001b[94m 0.560710\u001b[35m val_acc:\u001b[94m 82.464029\u001b[35m val_fscore_micro:\u001b[94m 0.824640\u001b[0m\n",
      "Epoch 28: val_acc improved from 81.29496 to 82.46403, saving file to model/embeddings_rnn\\checkpoint_epoch_28.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m29/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.20s \u001b[35mloss:\u001b[94m 0.430953\u001b[35m acc:\u001b[94m 86.296130\u001b[35m fscore_micro:\u001b[94m 0.862961\u001b[35m val_loss:\u001b[94m 0.567733\u001b[35m val_acc:\u001b[94m 80.935252\u001b[35m val_fscore_micro:\u001b[94m 0.809352\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m30/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.90s \u001b[35mloss:\u001b[94m 0.412025\u001b[35m acc:\u001b[94m 87.263726\u001b[35m fscore_micro:\u001b[94m 0.872637\u001b[35m val_loss:\u001b[94m 0.556089\u001b[35m val_acc:\u001b[94m 82.643885\u001b[35m val_fscore_micro:\u001b[94m 0.826439\u001b[0m\n",
      "Epoch 30: val_acc improved from 82.46403 to 82.64388, saving file to model/embeddings_rnn\\checkpoint_epoch_30.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m31/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.14s \u001b[35mloss:\u001b[94m 0.390565\u001b[35m acc:\u001b[94m 88.321332\u001b[35m fscore_micro:\u001b[94m 0.883213\u001b[35m val_loss:\u001b[94m 0.570223\u001b[35m val_acc:\u001b[94m 82.374101\u001b[35m val_fscore_micro:\u001b[94m 0.823741\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m32/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m13.95s \u001b[35mloss:\u001b[94m 0.381471\u001b[35m acc:\u001b[94m 88.321332\u001b[35m fscore_micro:\u001b[94m 0.883213\u001b[35m val_loss:\u001b[94m 0.581296\u001b[35m val_acc:\u001b[94m 81.025180\u001b[35m val_fscore_micro:\u001b[94m 0.810252\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m33/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.67s \u001b[35mloss:\u001b[94m 0.365309\u001b[35m acc:\u001b[94m 88.816382\u001b[35m fscore_micro:\u001b[94m 0.888164\u001b[35m val_loss:\u001b[94m 0.558108\u001b[35m val_acc:\u001b[94m 82.284173\u001b[35m val_fscore_micro:\u001b[94m 0.822842\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m34/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m18.07s \u001b[35mloss:\u001b[94m 0.347085\u001b[35m acc:\u001b[94m 89.851485\u001b[35m fscore_micro:\u001b[94m 0.898515\u001b[35m val_loss:\u001b[94m 0.574349\u001b[35m val_acc:\u001b[94m 81.564748\u001b[35m val_fscore_micro:\u001b[94m 0.815647\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m35/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.92s \u001b[35mloss:\u001b[94m 0.338898\u001b[35m acc:\u001b[94m 89.941494\u001b[35m fscore_micro:\u001b[94m 0.899415\u001b[35m val_loss:\u001b[94m 0.544307\u001b[35m val_acc:\u001b[94m 83.273381\u001b[35m val_fscore_micro:\u001b[94m 0.832734\u001b[0m\n",
      "Epoch 35: val_acc improved from 82.64388 to 83.27338, saving file to model/embeddings_rnn\\checkpoint_epoch_35.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m36/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m25.75s \u001b[35mloss:\u001b[94m 0.317247\u001b[35m acc:\u001b[94m 90.819082\u001b[35m fscore_micro:\u001b[94m 0.908191\u001b[35m val_loss:\u001b[94m 0.562676\u001b[35m val_acc:\u001b[94m 81.654676\u001b[35m val_fscore_micro:\u001b[94m 0.816547\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m37/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m25.54s \u001b[35mloss:\u001b[94m 0.309659\u001b[35m acc:\u001b[94m 90.684068\u001b[35m fscore_micro:\u001b[94m 0.906841\u001b[35m val_loss:\u001b[94m 0.633689\u001b[35m val_acc:\u001b[94m 80.935252\u001b[35m val_fscore_micro:\u001b[94m 0.809352\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m38/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m22.54s \u001b[35mloss:\u001b[94m 0.296188\u001b[35m acc:\u001b[94m 91.359136\u001b[35m fscore_micro:\u001b[94m 0.913591\u001b[35m val_loss:\u001b[94m 0.550582\u001b[35m val_acc:\u001b[94m 83.992806\u001b[35m val_fscore_micro:\u001b[94m 0.839928\u001b[0m\n",
      "Epoch 38: val_acc improved from 83.27338 to 83.99281, saving file to model/embeddings_rnn\\checkpoint_epoch_38.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m39/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m25.77s \u001b[35mloss:\u001b[94m 0.281192\u001b[35m acc:\u001b[94m 91.989199\u001b[35m fscore_micro:\u001b[94m 0.919892\u001b[35m val_loss:\u001b[94m 0.555230\u001b[35m val_acc:\u001b[94m 83.453237\u001b[35m val_fscore_micro:\u001b[94m 0.834532\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m40/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m26.18s \u001b[35mloss:\u001b[94m 0.275492\u001b[35m acc:\u001b[94m 91.876688\u001b[35m fscore_micro:\u001b[94m 0.918767\u001b[35m val_loss:\u001b[94m 0.569076\u001b[35m val_acc:\u001b[94m 81.834532\u001b[35m val_fscore_micro:\u001b[94m 0.818345\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m41/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m24.37s \u001b[35mloss:\u001b[94m 0.261137\u001b[35m acc:\u001b[94m 92.709271\u001b[35m fscore_micro:\u001b[94m 0.927093\u001b[35m val_loss:\u001b[94m 0.525537\u001b[35m val_acc:\u001b[94m 85.161871\u001b[35m val_fscore_micro:\u001b[94m 0.851619\u001b[0m\n",
      "Epoch 41: val_acc improved from 83.99281 to 85.16187, saving file to model/embeddings_rnn\\checkpoint_epoch_41.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m42/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m25.24s \u001b[35mloss:\u001b[94m 0.249005\u001b[35m acc:\u001b[94m 93.294329\u001b[35m fscore_micro:\u001b[94m 0.932943\u001b[35m val_loss:\u001b[94m 0.582676\u001b[35m val_acc:\u001b[94m 81.834532\u001b[35m val_fscore_micro:\u001b[94m 0.818345\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m43/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m29.56s \u001b[35mloss:\u001b[94m 0.232885\u001b[35m acc:\u001b[94m 93.609361\u001b[35m fscore_micro:\u001b[94m 0.936094\u001b[35m val_loss:\u001b[94m 0.534631\u001b[35m val_acc:\u001b[94m 83.902878\u001b[35m val_fscore_micro:\u001b[94m 0.839029\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m44/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m26.20s \u001b[35mloss:\u001b[94m 0.227764\u001b[35m acc:\u001b[94m 93.879388\u001b[35m fscore_micro:\u001b[94m 0.938794\u001b[35m val_loss:\u001b[94m 0.552022\u001b[35m val_acc:\u001b[94m 83.902878\u001b[35m val_fscore_micro:\u001b[94m 0.839029\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m45/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m24.42s \u001b[35mloss:\u001b[94m 0.221890\u001b[35m acc:\u001b[94m 93.744374\u001b[35m fscore_micro:\u001b[94m 0.937444\u001b[35m val_loss:\u001b[94m 0.573595\u001b[35m val_acc:\u001b[94m 82.284173\u001b[35m val_fscore_micro:\u001b[94m 0.822842\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m46/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m30.12s \u001b[35mloss:\u001b[94m 0.201079\u001b[35m acc:\u001b[94m 94.846985\u001b[35m fscore_micro:\u001b[94m 0.948470\u001b[35m val_loss:\u001b[94m 0.547538\u001b[35m val_acc:\u001b[94m 83.723022\u001b[35m val_fscore_micro:\u001b[94m 0.837230\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m47/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.96s \u001b[35mloss:\u001b[94m 0.195097\u001b[35m acc:\u001b[94m 95.072007\u001b[35m fscore_micro:\u001b[94m 0.950720\u001b[35m val_loss:\u001b[94m 0.549774\u001b[35m val_acc:\u001b[94m 83.453237\u001b[35m val_fscore_micro:\u001b[94m 0.834532\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m48/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.80s \u001b[35mloss:\u001b[94m 0.183964\u001b[35m acc:\u001b[94m 95.297030\u001b[35m fscore_micro:\u001b[94m 0.952970\u001b[35m val_loss:\u001b[94m 0.538694\u001b[35m val_acc:\u001b[94m 84.622302\u001b[35m val_fscore_micro:\u001b[94m 0.846223\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m49/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.47s \u001b[35mloss:\u001b[94m 0.178176\u001b[35m acc:\u001b[94m 95.117012\u001b[35m fscore_micro:\u001b[94m 0.951170\u001b[35m val_loss:\u001b[94m 0.750887\u001b[35m val_acc:\u001b[94m 77.068345\u001b[35m val_fscore_micro:\u001b[94m 0.770683\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m50/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m18.67s \u001b[35mloss:\u001b[94m 0.170986\u001b[35m acc:\u001b[94m 95.477048\u001b[35m fscore_micro:\u001b[94m 0.954771\u001b[35m val_loss:\u001b[94m 0.548540\u001b[35m val_acc:\u001b[94m 84.442446\u001b[35m val_fscore_micro:\u001b[94m 0.844424\u001b[0m\n",
      "Restoring model from model/embeddings_rnn\\checkpoint_epoch_41.ckpt\n"
     ]
    }
   ],
   "source": [
    "logging = experiment.train(train_dataloader, valid_dataloader, epochs=50, disable_tensorboard=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prédiction à l'aide du modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset_path = \"./data_rnn/test-questions-t3.txt\"\n",
    "x_test, test_labels = load_dataset(test_dataset_path)\n",
    "from numpy import argmax\n",
    "\n",
    "def obtain_prediction(sentence, label=None):\n",
    "    tokenized_sentence = [word2id.get(word.text,1) for word in nlp(sentence)]\n",
    "    sentence_length = len(tokenized_sentence)\n",
    "    class_score = model(LongTensor(tokenized_sentence).unsqueeze(0), LongTensor([sentence_length])).detach().numpy()\n",
    "    return id2lable[argmax(class_score)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is the capital of Yugoslavia ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: Where is Milan ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: What is the speed hummingbirds fly ?. \n",
      "Pred: QUANTITY, Truth: QUANTITY\n",
      "\n",
      "Q: What is the oldest city in the United States ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: What was W.C. Fields ' real name ?. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: What river flows between Fargo , North Dakota and Moorhead , Minnesota ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: What state did the Battle of Bighorn take place in ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: Who was Abraham Lincoln ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: What are spider veins ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: What day and month did John Lennon die ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n"
     ]
    }
   ],
   "source": [
    "def evaluate(x, y):\n",
    "    prediction = obtain_prediction(x)\n",
    "    print(\"\\nQ: {}. \\nPred: {}, Truth: {}\".format(x, prediction, y))\n",
    "\n",
    "for test_index in range(50, 60):\n",
    "    x = x_test[test_index]\n",
    "    y = test_labels[test_index]\n",
    "    evaluate(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Will Bernie Sanders ever become president. Pred:PERSON\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"Will Bernie Sanders ever become president\"\n",
    "print(\"Q: {}. Pred:{}\".format(new_sentence, obtain_prediction(new_sentence)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who let the dogs out. Pred:PERSON\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"Who let the dogs out\"\n",
    "print(\"Q: {}. Pred:{}\".format(new_sentence, obtain_prediction(new_sentence)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
