{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avec réseau récurrent, embeddings et mécanisme d'attention\n",
    "\n",
    "Cette exemple est très similaire au précedent, sauf qu'un mécanisme d'attention est utilisé pour déterminer l'importance relative des mots dans le processus de classification.\n",
    "\n",
    "On vous réfère au notebook \"Classification avec réseau récurrent pour les sections sur la préparation du jeu de données, du vocabulaire, des embeddings* et de l'entraînement du modèle.\n",
    "\n",
    "### 1. Création des jeux de données d'entraînement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"./data_rnn/questions-t3.txt\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        labels, questions = zip(*[tuple(s.split(' ', 1)) for s in lines])\n",
    "    return questions, labels\n",
    "\n",
    "questions, labels = load_dataset(train_dataset_path)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(questions, labels, test_size=0.2, shuffle=True,random_state=42)\n",
    "\n",
    "# On converti les labels textuels en index numérique\n",
    "id2lable = {label_id:value for label_id, value in enumerate(list(set(labels)))}\n",
    "label2id = {value:label_id for label_id, value in id2lable.items()}\n",
    "\n",
    "y_train = [label2id[label] for label in y_train]\n",
    "y_valid = [label2id[label] for label in y_valid]\n",
    "\n",
    "nb_class = len(id2lable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion du vocabulaire et des vecteurs des mots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "embedding_size = nlp.meta['vectors']['width']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2embedding = {}\n",
    "id2word = {}\n",
    "\n",
    "word2id[1] = \"<unk>\"\n",
    "\n",
    "id2embedding[1] = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "word_index = 2\n",
    "\n",
    "for question in X_train:\n",
    "    for word in nlp(question):\n",
    "        if word.text not in word2id.keys():\n",
    "            word2id[word.text] = word_index\n",
    "            id2embedding[word_index] = word.vector\n",
    "            id2word[word_index] = word.text\n",
    "            word_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import LongTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class TokenisedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset: List[str] , target: np.array, word2id: Dict[str, int], nlp_model):\n",
    "        self.tokenized_dataset = [None for _ in range(len(dataset))]\n",
    "        self.dataset = dataset\n",
    "        self.target = target\n",
    "        self.word2id = word2id\n",
    "        self.nlp_model = nlp_model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.tokenized_dataset[index] is None:\n",
    "            self.tokenized_dataset[index] = self.tokenize(self.dataset[index])\n",
    "        \n",
    "        return LongTensor(self.tokenized_dataset[index]), LongTensor([self.target[index]]).squeeze(0)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        return [ self.word2id.get(word.text, 1) for word in self.nlp_model(sentence)]\n",
    "    \n",
    "    \n",
    "train_dataset = TokenisedDataset(X_train, y_train, word2id, nlp)\n",
    "valid_dataset = TokenisedDataset(X_valid, y_valid, word2id, nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construction de l'architecture neuronale\n",
    "L'architecture du réseau récurrent comporte toujours:\n",
    "\n",
    "* une couche en entrée qui prend les embeddings de mots de Spacy. La taille de la couche d'entrée correspond à la taille d'embedding de Spacy.\n",
    "* une couche cachée récurrent qui prend en entrée un embedding de mot et l'état caché précédent. Les neurones de cette couche sont de type LSTM, une structure de neurone qui facilite la propagation d'information sur de plus longues séquences. À noer que la couche est bi-directionnelle (voir note de cours).\n",
    "* une couche de classification qui donne en sortie un score pour chacune des classes (types de question).\n",
    "\n",
    "On ajoute cependant une couche d'attention qui est une couche linéaire qui donne un poids d'attention à chacun des mots de la question. Ces poids contribuent autant à la classification des questions que pour évaluer l'importance relative des mots.\n",
    "\n",
    "La partie important est la méthode _handle_rnn_output qui gère le calcul des poids d'attention et qui crée un vecteur (une somme pondérée des états cachées pondérés par les poids d'attention des mots) utilisé pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "NEG_INF = -1e6\n",
    "\n",
    "class AttentionRNNWithEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_state_size, nb_class) :\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding)\n",
    "        embedding_size = embedding.size()[1]\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_state_size, 1, bidirectional=True)        \n",
    "        self.attention_layer = nn.Linear(2 * hidden_state_size, 1) # On calcule un facteur (scalaire) par input\n",
    "        self.classification_layer = nn.Linear(2 * hidden_state_size, nb_class) # 2 * -> Une pour chaque direction\n",
    "    \n",
    "    def forward(self, x, x_lenghts):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self._handle_rnn_output(x, x_lenghts)\n",
    "        x = self.classification_layer(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def _handle_rnn_output(self, x, x_lenghts):\n",
    "        \n",
    "        # On \"pack\" les batch pour les envoyer dans le RNN\n",
    "        packed_batch = pack_padded_sequence(x, x_lenghts, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # On s'intéresse cette fois-ci aux outputs après chaque mots\n",
    "        rnn_output, _ = self.rnn(packed_batch)\n",
    "        \n",
    "        # On \"repad\" les outputs pour les remettre dans une forme utilisable\n",
    "        unpacked_rnn_output, _ = pad_packed_sequence(rnn_output, batch_first=True)\n",
    "\n",
    "        # On génère un masque pour prévenir que des poids d'attention soient calculés sur le padding\n",
    "        sequence_mask = self.make_sequence_mask(x_lenghts)\n",
    "        \n",
    "        # On calcule les poids d'attention pour les outputs du RNN \n",
    "        attention = self.attention_layer(unpacked_rnn_output)\n",
    "        \n",
    "        # On normalize les poids d'attention\n",
    "        soft_maxed_attention = self.mask_softmax(attention.squeeze(-1), sequence_mask)\n",
    "        \n",
    "        # On pondère les outputs du RNN avec les poids d'attention\n",
    "        attention_weighted_rnn_output = torch.sum(soft_maxed_attention.unsqueeze(-1) * unpacked_rnn_output, dim=1)\n",
    "\n",
    "        return attention_weighted_rnn_output\n",
    "        \n",
    "    def calculate_attention_for_input(self, x, x_lenghts):\n",
    "        x = self.embedding_layer(x)\n",
    "        packed_batch = pack_padded_sequence(x, x_lenghts, batch_first=True, enforce_sorted=False)\n",
    "        rnn_output, _ = self.rnn(packed_batch)\n",
    "        unpacked_rnn_output, _ = pad_packed_sequence(rnn_output, batch_first=True)\n",
    "        sequence_mask = self.make_sequence_mask(x_lenghts)\n",
    "        attention = self.attention_layer(unpacked_rnn_output)\n",
    "        soft_maxed_attention = self.mask_softmax(attention.squeeze(-1), sequence_mask)\n",
    "        return soft_maxed_attention\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def make_sequence_mask(sequence_lengths):\n",
    "        maximum_length = torch.max(sequence_lengths)\n",
    "\n",
    "        idx = torch.arange(maximum_length).to(sequence_lengths).repeat(sequence_lengths.size(0), 1)\n",
    "        mask = torch.gt(sequence_lengths.unsqueeze(-1), idx).to(sequence_lengths)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask_softmax(matrix, mask=None):\n",
    "        if mask is None:\n",
    "            result = nn.functional.softmax(matrix, dim=-1)\n",
    "        else:\n",
    "            mask_norm = ((1 - mask) * NEG_INF).to(matrix)\n",
    "            for i in range(matrix.dim() - mask_norm.dim()):\n",
    "                mask_norm = mask_norm.unsqueeze(1)\n",
    "            result = nn.functional.softmax(matrix + mask_norm, dim=-1)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch : List[Tuple[LongTensor, LongTensor]]) -> Tuple[LongTensor, LongTensor]:\n",
    "    x = [x for x,y in batch]\n",
    "    x_true_length = [len(x) for x,y in batch]\n",
    "    y = torch.stack([y for x,y in batch], dim=0)\n",
    "    \n",
    "    return ((pad_sequence(x, batch_first=True), LongTensor(x_true_length)), y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2embedding[0] = np.zeros(embedding_size, dtype=np.float32)\n",
    "embedding_layer = np.zeros((len(id2embedding), embedding_size), dtype=np.float32)\n",
    "for token_index, embedding in id2embedding.items():\n",
    "    embedding_layer[token_index,:] = embedding\n",
    "    \n",
    "embedding_layer = torch.from_numpy(embedding_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement du modèle\n",
    "\n",
    "Rien de bien nouveau ici sous le soleil de Poutyne. Vous êtes en terrain familier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "import numpy as np\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100\n",
    "\n",
    "model = AttentionRNNWithEmbeddingLayer(embedding_layer, hidden_size, nb_class)\n",
    "experiment = Experiment('model/attention_embeddings_rnn', \n",
    "                        model, \n",
    "                        optimizer = \"SGD\", \n",
    "                        task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m1/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m92.86s \u001b[35mloss:\u001b[94m 2.103834\u001b[35m acc:\u001b[94m 19.801980\u001b[35m fscore_micro:\u001b[94m 0.198020\u001b[35m val_loss:\u001b[94m 2.027687\u001b[35m val_acc:\u001b[94m 22.571942\u001b[35m val_fscore_micro:\u001b[94m 0.225719\u001b[0m\n",
      "Epoch 1: val_acc improved from -inf to 22.57194, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_1.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m2/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.07s \u001b[35mloss:\u001b[94m 2.009135\u001b[35m acc:\u001b[94m 22.479748\u001b[35m fscore_micro:\u001b[94m 0.224797\u001b[35m val_loss:\u001b[94m 1.974789\u001b[35m val_acc:\u001b[94m 22.571942\u001b[35m val_fscore_micro:\u001b[94m 0.225719\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m3/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.66s \u001b[35mloss:\u001b[94m 1.975864\u001b[35m acc:\u001b[94m 22.479748\u001b[35m fscore_micro:\u001b[94m 0.224797\u001b[35m val_loss:\u001b[94m 1.948941\u001b[35m val_acc:\u001b[94m 22.571942\u001b[35m val_fscore_micro:\u001b[94m 0.225719\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m4/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.10s \u001b[35mloss:\u001b[94m 1.955877\u001b[35m acc:\u001b[94m 22.682268\u001b[35m fscore_micro:\u001b[94m 0.226823\u001b[35m val_loss:\u001b[94m 1.928807\u001b[35m val_acc:\u001b[94m 23.291367\u001b[35m val_fscore_micro:\u001b[94m 0.232914\u001b[0m\n",
      "Epoch 4: val_acc improved from 22.57194 to 23.29137, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_4.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m5/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.25s \u001b[35mloss:\u001b[94m 1.937099\u001b[35m acc:\u001b[94m 23.762376\u001b[35m fscore_micro:\u001b[94m 0.237624\u001b[35m val_loss:\u001b[94m 1.908803\u001b[35m val_acc:\u001b[94m 24.100719\u001b[35m val_fscore_micro:\u001b[94m 0.241007\u001b[0m\n",
      "Epoch 5: val_acc improved from 23.29137 to 24.10072, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_5.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m6/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.84s \u001b[35mloss:\u001b[94m 1.916218\u001b[35m acc:\u001b[94m 26.147615\u001b[35m fscore_micro:\u001b[94m 0.261476\u001b[35m val_loss:\u001b[94m 1.887082\u001b[35m val_acc:\u001b[94m 29.136691\u001b[35m val_fscore_micro:\u001b[94m 0.291367\u001b[0m\n",
      "Epoch 6: val_acc improved from 24.10072 to 29.13669, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_6.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m7/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.06s \u001b[35mloss:\u001b[94m 1.890677\u001b[35m acc:\u001b[94m 29.995500\u001b[35m fscore_micro:\u001b[94m 0.299955\u001b[35m val_loss:\u001b[94m 1.859510\u001b[35m val_acc:\u001b[94m 35.341727\u001b[35m val_fscore_micro:\u001b[94m 0.353417\u001b[0m\n",
      "Epoch 7: val_acc improved from 29.13669 to 35.34173, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_7.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m8/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.76s \u001b[35mloss:\u001b[94m 1.858268\u001b[35m acc:\u001b[94m 33.640864\u001b[35m fscore_micro:\u001b[94m 0.336409\u001b[35m val_loss:\u001b[94m 1.823244\u001b[35m val_acc:\u001b[94m 37.859712\u001b[35m val_fscore_micro:\u001b[94m 0.378597\u001b[0m\n",
      "Epoch 8: val_acc improved from 35.34173 to 37.85971, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_8.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m9/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m18.52s \u001b[35mloss:\u001b[94m 1.815355\u001b[35m acc:\u001b[94m 37.601260\u001b[35m fscore_micro:\u001b[94m 0.376013\u001b[35m val_loss:\u001b[94m 1.772634\u001b[35m val_acc:\u001b[94m 39.298561\u001b[35m val_fscore_micro:\u001b[94m 0.392986\u001b[0m\n",
      "Epoch 9: val_acc improved from 37.85971 to 39.29856, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_9.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m10/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.72s \u001b[35mloss:\u001b[94m 1.756549\u001b[35m acc:\u001b[94m 40.639064\u001b[35m fscore_micro:\u001b[94m 0.406391\u001b[35m val_loss:\u001b[94m 1.704313\u001b[35m val_acc:\u001b[94m 41.996403\u001b[35m val_fscore_micro:\u001b[94m 0.419964\u001b[0m\n",
      "Epoch 10: val_acc improved from 39.29856 to 41.99640, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_10.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m11/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.40s \u001b[35mloss:\u001b[94m 1.678601\u001b[35m acc:\u001b[94m 43.699370\u001b[35m fscore_micro:\u001b[94m 0.436994\u001b[35m val_loss:\u001b[94m 1.615575\u001b[35m val_acc:\u001b[94m 48.201439\u001b[35m val_fscore_micro:\u001b[94m 0.482014\u001b[0m\n",
      "Epoch 11: val_acc improved from 41.99640 to 48.20144, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_11.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m12/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.71s \u001b[35mloss:\u001b[94m 1.587710\u001b[35m acc:\u001b[94m 49.144915\u001b[35m fscore_micro:\u001b[94m 0.491449\u001b[35m val_loss:\u001b[94m 1.524207\u001b[35m val_acc:\u001b[94m 53.057554\u001b[35m val_fscore_micro:\u001b[94m 0.530576\u001b[0m\n",
      "Epoch 12: val_acc improved from 48.20144 to 53.05755, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_12.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m13/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.33s \u001b[35mloss:\u001b[94m 1.499701\u001b[35m acc:\u001b[94m 53.847885\u001b[35m fscore_micro:\u001b[94m 0.538479\u001b[35m val_loss:\u001b[94m 1.440562\u001b[35m val_acc:\u001b[94m 57.374101\u001b[35m val_fscore_micro:\u001b[94m 0.573741\u001b[0m\n",
      "Epoch 13: val_acc improved from 53.05755 to 57.37410, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_13.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m14/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m25.36s \u001b[35mloss:\u001b[94m 1.417775\u001b[35m acc:\u001b[94m 56.908191\u001b[35m fscore_micro:\u001b[94m 0.569082\u001b[35m val_loss:\u001b[94m 1.370355\u001b[35m val_acc:\u001b[94m 61.510791\u001b[35m val_fscore_micro:\u001b[94m 0.615108\u001b[0m\n",
      "Epoch 14: val_acc improved from 57.37410 to 61.51079, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_14.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m15/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.47s \u001b[35mloss:\u001b[94m 1.338430\u001b[35m acc:\u001b[94m 59.923492\u001b[35m fscore_micro:\u001b[94m 0.599235\u001b[35m val_loss:\u001b[94m 1.292146\u001b[35m val_acc:\u001b[94m 63.219424\u001b[35m val_fscore_micro:\u001b[94m 0.632194\u001b[0m\n",
      "Epoch 15: val_acc improved from 61.51079 to 63.21942, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_15.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m16/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.18s \u001b[35mloss:\u001b[94m 1.255743\u001b[35m acc:\u001b[94m 62.668767\u001b[35m fscore_micro:\u001b[94m 0.626688\u001b[35m val_loss:\u001b[94m 1.215500\u001b[35m val_acc:\u001b[94m 63.758993\u001b[35m val_fscore_micro:\u001b[94m 0.637590\u001b[0m\n",
      "Epoch 16: val_acc improved from 63.21942 to 63.75899, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_16.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m17/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.03s \u001b[35mloss:\u001b[94m 1.176770\u001b[35m acc:\u001b[94m 64.333933\u001b[35m fscore_micro:\u001b[94m 0.643339\u001b[35m val_loss:\u001b[94m 1.144071\u001b[35m val_acc:\u001b[94m 64.028777\u001b[35m val_fscore_micro:\u001b[94m 0.640288\u001b[0m\n",
      "Epoch 17: val_acc improved from 63.75899 to 64.02878, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_17.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m18/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.22s \u001b[35mloss:\u001b[94m 1.105506\u001b[35m acc:\u001b[94m 65.751575\u001b[35m fscore_micro:\u001b[94m 0.657516\u001b[35m val_loss:\u001b[94m 1.085634\u001b[35m val_acc:\u001b[94m 65.737410\u001b[35m val_fscore_micro:\u001b[94m 0.657374\u001b[0m\n",
      "Epoch 18: val_acc improved from 64.02878 to 65.73741, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_18.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m19/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.85s \u001b[35mloss:\u001b[94m 1.044757\u001b[35m acc:\u001b[94m 67.056706\u001b[35m fscore_micro:\u001b[94m 0.670567\u001b[35m val_loss:\u001b[94m 1.035391\u001b[35m val_acc:\u001b[94m 67.176259\u001b[35m val_fscore_micro:\u001b[94m 0.671763\u001b[0m\n",
      "Epoch 19: val_acc improved from 65.73741 to 67.17626, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_19.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m20/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.28s \u001b[35mloss:\u001b[94m 0.993167\u001b[35m acc:\u001b[94m 68.564356\u001b[35m fscore_micro:\u001b[94m 0.685644\u001b[35m val_loss:\u001b[94m 0.988446\u001b[35m val_acc:\u001b[94m 68.884892\u001b[35m val_fscore_micro:\u001b[94m 0.688849\u001b[0m\n",
      "Epoch 20: val_acc improved from 67.17626 to 68.88489, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_20.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m21/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.24s \u001b[35mloss:\u001b[94m 0.947490\u001b[35m acc:\u001b[94m 70.049505\u001b[35m fscore_micro:\u001b[94m 0.700495\u001b[35m val_loss:\u001b[94m 0.953427\u001b[35m val_acc:\u001b[94m 70.053957\u001b[35m val_fscore_micro:\u001b[94m 0.700540\u001b[0m\n",
      "Epoch 21: val_acc improved from 68.88489 to 70.05396, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_21.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m22/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.06s \u001b[35mloss:\u001b[94m 0.907809\u001b[35m acc:\u001b[94m 71.354635\u001b[35m fscore_micro:\u001b[94m 0.713546\u001b[35m val_loss:\u001b[94m 0.911133\u001b[35m val_acc:\u001b[94m 72.661871\u001b[35m val_fscore_micro:\u001b[94m 0.726619\u001b[0m\n",
      "Epoch 22: val_acc improved from 70.05396 to 72.66187, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_22.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m23/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.43s \u001b[35mloss:\u001b[94m 0.869179\u001b[35m acc:\u001b[94m 73.199820\u001b[35m fscore_micro:\u001b[94m 0.731998\u001b[35m val_loss:\u001b[94m 0.879054\u001b[35m val_acc:\u001b[94m 72.302158\u001b[35m val_fscore_micro:\u001b[94m 0.723022\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m24/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.00s \u001b[35mloss:\u001b[94m 0.833481\u001b[35m acc:\u001b[94m 74.099910\u001b[35m fscore_micro:\u001b[94m 0.740999\u001b[35m val_loss:\u001b[94m 0.853276\u001b[35m val_acc:\u001b[94m 73.830935\u001b[35m val_fscore_micro:\u001b[94m 0.738309\u001b[0m\n",
      "Epoch 24: val_acc improved from 72.66187 to 73.83094, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_24.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m25/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.74s \u001b[35mloss:\u001b[94m 0.800587\u001b[35m acc:\u001b[94m 75.135014\u001b[35m fscore_micro:\u001b[94m 0.751350\u001b[35m val_loss:\u001b[94m 0.819728\u001b[35m val_acc:\u001b[94m 74.550360\u001b[35m val_fscore_micro:\u001b[94m 0.745504\u001b[0m\n",
      "Epoch 25: val_acc improved from 73.83094 to 74.55036, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_25.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m26/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.88s \u001b[35mloss:\u001b[94m 0.767177\u001b[35m acc:\u001b[94m 75.900090\u001b[35m fscore_micro:\u001b[94m 0.759001\u001b[35m val_loss:\u001b[94m 0.801294\u001b[35m val_acc:\u001b[94m 74.730216\u001b[35m val_fscore_micro:\u001b[94m 0.747302\u001b[0m\n",
      "Epoch 26: val_acc improved from 74.55036 to 74.73022, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_26.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m27/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.75s \u001b[35mloss:\u001b[94m 0.736923\u001b[35m acc:\u001b[94m 76.597660\u001b[35m fscore_micro:\u001b[94m 0.765977\u001b[35m val_loss:\u001b[94m 0.762541\u001b[35m val_acc:\u001b[94m 75.899281\u001b[35m val_fscore_micro:\u001b[94m 0.758993\u001b[0m\n",
      "Epoch 27: val_acc improved from 74.73022 to 75.89928, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_27.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m28/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.01s \u001b[35mloss:\u001b[94m 0.710873\u001b[35m acc:\u001b[94m 77.385239\u001b[35m fscore_micro:\u001b[94m 0.773852\u001b[35m val_loss:\u001b[94m 0.749354\u001b[35m val_acc:\u001b[94m 75.899281\u001b[35m val_fscore_micro:\u001b[94m 0.758993\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m29/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.36s \u001b[35mloss:\u001b[94m 0.688001\u001b[35m acc:\u001b[94m 77.947795\u001b[35m fscore_micro:\u001b[94m 0.779478\u001b[35m val_loss:\u001b[94m 0.725920\u001b[35m val_acc:\u001b[94m 76.888489\u001b[35m val_fscore_micro:\u001b[94m 0.768885\u001b[0m\n",
      "Epoch 29: val_acc improved from 75.89928 to 76.88849, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_29.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m30/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.90s \u001b[35mloss:\u001b[94m 0.662574\u001b[35m acc:\u001b[94m 78.937894\u001b[35m fscore_micro:\u001b[94m 0.789379\u001b[35m val_loss:\u001b[94m 0.700754\u001b[35m val_acc:\u001b[94m 77.068345\u001b[35m val_fscore_micro:\u001b[94m 0.770683\u001b[0m\n",
      "Epoch 30: val_acc improved from 76.88849 to 77.06835, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_30.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m31/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.11s \u001b[35mloss:\u001b[94m 0.638025\u001b[35m acc:\u001b[94m 79.792979\u001b[35m fscore_micro:\u001b[94m 0.797930\u001b[35m val_loss:\u001b[94m 0.689614\u001b[35m val_acc:\u001b[94m 76.978417\u001b[35m val_fscore_micro:\u001b[94m 0.769784\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m32/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.10s \u001b[35mloss:\u001b[94m 0.617287\u001b[35m acc:\u001b[94m 79.995500\u001b[35m fscore_micro:\u001b[94m 0.799955\u001b[35m val_loss:\u001b[94m 0.717641\u001b[35m val_acc:\u001b[94m 77.248201\u001b[35m val_fscore_micro:\u001b[94m 0.772482\u001b[0m\n",
      "Epoch 32: val_acc improved from 77.06835 to 77.24820, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_32.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m33/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.08s \u001b[35mloss:\u001b[94m 0.597938\u001b[35m acc:\u001b[94m 81.143114\u001b[35m fscore_micro:\u001b[94m 0.811431\u001b[35m val_loss:\u001b[94m 0.918537\u001b[35m val_acc:\u001b[94m 70.503597\u001b[35m val_fscore_micro:\u001b[94m 0.705036\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m34/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.09s \u001b[35mloss:\u001b[94m 0.577490\u001b[35m acc:\u001b[94m 81.165617\u001b[35m fscore_micro:\u001b[94m 0.811656\u001b[35m val_loss:\u001b[94m 0.640633\u001b[35m val_acc:\u001b[94m 79.406475\u001b[35m val_fscore_micro:\u001b[94m 0.794065\u001b[0m\n",
      "Epoch 34: val_acc improved from 77.24820 to 79.40647, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_34.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m35/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.35s \u001b[35mloss:\u001b[94m 0.559628\u001b[35m acc:\u001b[94m 81.975698\u001b[35m fscore_micro:\u001b[94m 0.819757\u001b[35m val_loss:\u001b[94m 0.619859\u001b[35m val_acc:\u001b[94m 80.665468\u001b[35m val_fscore_micro:\u001b[94m 0.806655\u001b[0m\n",
      "Epoch 35: val_acc improved from 79.40647 to 80.66547, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_35.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m36/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.81s \u001b[35mloss:\u001b[94m 0.543376\u001b[35m acc:\u001b[94m 82.560756\u001b[35m fscore_micro:\u001b[94m 0.825608\u001b[35m val_loss:\u001b[94m 0.604170\u001b[35m val_acc:\u001b[94m 80.845324\u001b[35m val_fscore_micro:\u001b[94m 0.808453\u001b[0m\n",
      "Epoch 36: val_acc improved from 80.66547 to 80.84532, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_36.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m37/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.05s \u001b[35mloss:\u001b[94m 0.526795\u001b[35m acc:\u001b[94m 82.920792\u001b[35m fscore_micro:\u001b[94m 0.829208\u001b[35m val_loss:\u001b[94m 0.607483\u001b[35m val_acc:\u001b[94m 80.395683\u001b[35m val_fscore_micro:\u001b[94m 0.803957\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m38/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.04s \u001b[35mloss:\u001b[94m 0.505822\u001b[35m acc:\u001b[94m 84.270927\u001b[35m fscore_micro:\u001b[94m 0.842709\u001b[35m val_loss:\u001b[94m 0.648763\u001b[35m val_acc:\u001b[94m 79.316547\u001b[35m val_fscore_micro:\u001b[94m 0.793165\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m39/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m18.60s \u001b[35mloss:\u001b[94m 0.492800\u001b[35m acc:\u001b[94m 84.090909\u001b[35m fscore_micro:\u001b[94m 0.840909\u001b[35m val_loss:\u001b[94m 0.582506\u001b[35m val_acc:\u001b[94m 81.294964\u001b[35m val_fscore_micro:\u001b[94m 0.812950\u001b[0m\n",
      "Epoch 39: val_acc improved from 80.84532 to 81.29496, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_39.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m40/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m16.49s \u001b[35mloss:\u001b[94m 0.482397\u001b[35m acc:\u001b[94m 84.698470\u001b[35m fscore_micro:\u001b[94m 0.846985\u001b[35m val_loss:\u001b[94m 0.600735\u001b[35m val_acc:\u001b[94m 81.025180\u001b[35m val_fscore_micro:\u001b[94m 0.810252\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m41/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m17.80s \u001b[35mloss:\u001b[94m 0.463400\u001b[35m acc:\u001b[94m 85.238524\u001b[35m fscore_micro:\u001b[94m 0.852385\u001b[35m val_loss:\u001b[94m 0.596642\u001b[35m val_acc:\u001b[94m 82.014388\u001b[35m val_fscore_micro:\u001b[94m 0.820144\u001b[0m\n",
      "Epoch 41: val_acc improved from 81.29496 to 82.01439, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_41.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m42/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m19.39s \u001b[35mloss:\u001b[94m 0.453051\u001b[35m acc:\u001b[94m 85.373537\u001b[35m fscore_micro:\u001b[94m 0.853735\u001b[35m val_loss:\u001b[94m 0.644557\u001b[35m val_acc:\u001b[94m 78.866906\u001b[35m val_fscore_micro:\u001b[94m 0.788669\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m43/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.69s \u001b[35mloss:\u001b[94m 0.445752\u001b[35m acc:\u001b[94m 85.801080\u001b[35m fscore_micro:\u001b[94m 0.858011\u001b[35m val_loss:\u001b[94m 0.579640\u001b[35m val_acc:\u001b[94m 81.205036\u001b[35m val_fscore_micro:\u001b[94m 0.812050\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m44/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.28s \u001b[35mloss:\u001b[94m 0.429443\u001b[35m acc:\u001b[94m 86.273627\u001b[35m fscore_micro:\u001b[94m 0.862736\u001b[35m val_loss:\u001b[94m 0.601460\u001b[35m val_acc:\u001b[94m 81.924460\u001b[35m val_fscore_micro:\u001b[94m 0.819245\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m45/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m13.91s \u001b[35mloss:\u001b[94m 0.418254\u001b[35m acc:\u001b[94m 86.858686\u001b[35m fscore_micro:\u001b[94m 0.868587\u001b[35m val_loss:\u001b[94m 0.542912\u001b[35m val_acc:\u001b[94m 82.464029\u001b[35m val_fscore_micro:\u001b[94m 0.824640\u001b[0m\n",
      "Epoch 45: val_acc improved from 82.01439 to 82.46403, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_45.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m46/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.04s \u001b[35mloss:\u001b[94m 0.402377\u001b[35m acc:\u001b[94m 87.128713\u001b[35m fscore_micro:\u001b[94m 0.871287\u001b[35m val_loss:\u001b[94m 0.546161\u001b[35m val_acc:\u001b[94m 83.183453\u001b[35m val_fscore_micro:\u001b[94m 0.831835\u001b[0m\n",
      "Epoch 46: val_acc improved from 82.46403 to 83.18345, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_46.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m47/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.53s \u001b[35mloss:\u001b[94m 0.395512\u001b[35m acc:\u001b[94m 87.758776\u001b[35m fscore_micro:\u001b[94m 0.877588\u001b[35m val_loss:\u001b[94m 0.537136\u001b[35m val_acc:\u001b[94m 83.633094\u001b[35m val_fscore_micro:\u001b[94m 0.836331\u001b[0m\n",
      "Epoch 47: val_acc improved from 83.18345 to 83.63309, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_47.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m48/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.58s \u001b[35mloss:\u001b[94m 0.381333\u001b[35m acc:\u001b[94m 88.568857\u001b[35m fscore_micro:\u001b[94m 0.885689\u001b[35m val_loss:\u001b[94m 0.528303\u001b[35m val_acc:\u001b[94m 83.453237\u001b[35m val_fscore_micro:\u001b[94m 0.834532\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m49/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m14.70s \u001b[35mloss:\u001b[94m 0.372996\u001b[35m acc:\u001b[94m 88.703870\u001b[35m fscore_micro:\u001b[94m 0.887039\u001b[35m val_loss:\u001b[94m 0.564952\u001b[35m val_acc:\u001b[94m 81.564748\u001b[35m val_fscore_micro:\u001b[94m 0.815647\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m50/50 \u001b[35mStep: \u001b[36m278/278 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m15.21s \u001b[35mloss:\u001b[94m 0.366609\u001b[35m acc:\u001b[94m 88.771377\u001b[35m fscore_micro:\u001b[94m 0.887714\u001b[35m val_loss:\u001b[94m 0.525839\u001b[35m val_acc:\u001b[94m 84.082734\u001b[35m val_fscore_micro:\u001b[94m 0.840827\u001b[0m\n",
      "Epoch 50: val_acc improved from 83.63309 to 84.08273, saving file to model/attention_embeddings_rnn\\checkpoint_epoch_50.ckpt\n",
      "Restoring model from model/attention_embeddings_rnn\\checkpoint_epoch_50.ckpt\n"
     ]
    }
   ],
   "source": [
    "logging = experiment.train(train_dataloader, valid_dataloader, epochs=50, disable_tensorboard=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prédiction à l'aide du modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = \"./data_rnn/test-questions-t3.txt\"\n",
    "x_test, test_labels = load_dataset(test_dataset_path)\n",
    "from numpy import argmax\n",
    "\n",
    "def obtain_prediction(sentence, label=None):\n",
    "    tokenized_sentence = [word2id.get(word.text,1) for word in nlp(sentence)]\n",
    "    sentence_length = len(tokenized_sentence)\n",
    "    class_score = model(LongTensor(tokenized_sentence).unsqueeze(0), LongTensor([sentence_length])).detach().numpy()\n",
    "    return id2lable[argmax(class_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was the last year that the Chicago Cubs won the World Series ?. Pred:TEMPORAL, Truth:TEMPORAL\n"
     ]
    }
   ],
   "source": [
    "test_index = 101\n",
    "\n",
    "print(\"Q: {}. Pred:{}, Truth:{}\".\n",
    "      format(x_test[test_index], obtain_prediction(x_test[test_index]), test_labels[test_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Will Bernie Sanders ever become president. Pred:ENTITY\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"Will Bernie Sanders ever become president\"\n",
    "print(\"Q: {}. Pred:{}\".format(new_sentence, obtain_prediction(new_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is desktop publishing ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: What is the temperature of the sun 's surface ?. \n",
      "Pred: QUANTITY, Truth: QUANTITY\n",
      "\n",
      "Q: What year did Canada join the United Nations ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n",
      "\n",
      "Q: Where is Prince Edward Island ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: Mercury , what year was it discovered ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n",
      "\n",
      "Q: What is cryogenics ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: What are coral reefs ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: What is neurology ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: Who invented the calculator ?. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: How do you measure earthquakes ?. \n",
      "Pred: DESCRIPTION, Truth: DEFINITION\n",
      "\n",
      "Q: Who is Duke Ellington ?. \n",
      "Pred: PERSON, Truth: DEFINITION\n",
      "\n",
      "Q: What county is Phoenix , AZ in ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: What is a micron ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: The sun 's core , what is the temperature ?. \n",
      "Pred: DESCRIPTION, Truth: QUANTITY\n",
      "\n",
      "Q: When were William Shakespeare 's twins born ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n",
      "\n",
      "Q: What is the highest dam in the U.S. ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: What is acupuncture ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: What is the length of the coastline of the state of Alaska ?. \n",
      "Pred: QUANTITY, Truth: QUANTITY\n",
      "\n",
      "Q: What is the name of Neil Armstrong 's wife ?. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: Who won Ms. American in 1989 ?. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: When did the Hindenberg crash ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n",
      "\n",
      "Q: What was the last year that the Chicago Cubs won the World Series ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n",
      "\n",
      "Q: Where is Perth ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n",
      "\n",
      "Q: What year did WWII begin ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n",
      "\n",
      "Q: How did serfdom develop in and then leave Russia ?. \n",
      "Pred: DESCRIPTION, Truth: DESCRIPTION\n",
      "\n",
      "Q: What films featured the character Popeye Doyle ?. \n",
      "Pred: ENTITY, Truth: ENTITY\n",
      "\n",
      "Q: How can I find a list of celebrities ' real names ?. \n",
      "Pred: DESCRIPTION, Truth: DESCRIPTION\n",
      "\n",
      "Q: What fowl grabs the spotlight after the Chinese Year of the Monkey ?. \n",
      "Pred: TEMPORAL, Truth: ENTITY\n",
      "\n",
      "Q: What is the full form of .com ?. \n",
      "Pred: ENTITY, Truth: ABBREVIATION\n",
      "\n",
      "Q: What contemptible scoundrel stole the cork from my lunch ?. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: What team did baseball 's St. Louis Browns become ?. \n",
      "Pred: ORGANIZATION, Truth: ORGANIZATION\n",
      "\n",
      "Q: What is the oldest profession ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: What are liver enzymes ?. \n",
      "Pred: DEFINITION, Truth: DEFINITION\n",
      "\n",
      "Q: Name the scar-faced bounty hunter of The Old West .. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: When was Ozzy Osbourne born ?. \n",
      "Pred: TEMPORAL, Truth: TEMPORAL\n",
      "\n",
      "Q: Why do heavier objects travel downhill faster ?. \n",
      "Pred: DESCRIPTION, Truth: DESCRIPTION\n",
      "\n",
      "Q: Who was The Pride of the Yankees ?. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: Who killed Gandhi ?. \n",
      "Pred: PERSON, Truth: PERSON\n",
      "\n",
      "Q: What is considered the costliest disaster the insurance industry has ever faced ?. \n",
      "Pred: ENTITY, Truth: ENTITY\n",
      "\n",
      "Q: What sprawling U.S. state boasts the most airports ?. \n",
      "Pred: LOCATION, Truth: LOCATION\n"
     ]
    }
   ],
   "source": [
    "def evaluate(x, y):\n",
    "    prediction = obtain_prediction(x)\n",
    "    print(\"\\nQ: {}. \\nPred: {}, Truth: {}\".format(x, prediction, y))\n",
    "\n",
    "for test_index in range(80, 120):\n",
    "    x = x_test[test_index]\n",
    "    y = test_labels[test_index]\n",
    "    evaluate(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explication de la prédiction grâce à l'attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_attention(sentence, label=None):\n",
    "    tokenized_sentence = [word2id.get(word.text,1) for word in nlp(sentence)]\n",
    "    sentence_length = len(tokenized_sentence)\n",
    "    attention = model.calculate_attention_for_input(LongTensor(tokenized_sentence).unsqueeze(0), LongTensor([sentence_length])).squeeze(0).detach().numpy()\n",
    "    return list(zip(nlp(sentence), attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(What, 0.17246982),\n",
       " (is, 0.32196993),\n",
       " (cryogenics, 0.45657015),\n",
       " (?, 0.04899015)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 85\n",
    "obtain_attention(x_test[test_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(How, 0.29325584),\n",
       " (do, 0.13895506),\n",
       " (you, 0.08447854),\n",
       " (measure, 0.19434215),\n",
       " (earthquakes, 0.24090087),\n",
       " (?, 0.048067585)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 89\n",
    "obtain_attention(x_test[test_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(What, 0.006417403),\n",
       " (year, 0.9671106),\n",
       " (did, 0.01588893),\n",
       " (WWII, 0.0077822166),\n",
       " (begin, 0.002256962),\n",
       " (?, 0.00054381165)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 103\n",
    "obtain_attention(x_test[test_index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
