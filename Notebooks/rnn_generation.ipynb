{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de génération de texte avec un réseau récurrent\n",
    "On reprend ici la tâche de génération de noms de famille pour illuster l'utilisation d'un réseau récurrent pour la génération de textes. Les données utilisées contiennent des noms dans 18 langues d'origine. À partir de ces exemples, on entraîne un seul modèle RNN pour toutes les langues.\n",
    "\n",
    "### 1. Préparation du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_filename = \"./data/train_names.json\"\n",
    "test_filename = './data/test-names-t2.txt'\n",
    "\n",
    "names_by_origin = {}  # un dictionnaire qui contient une liste de noms pour chaque langue d'origine\n",
    "all_origins = []  # la liste des 18 langue d'origines de noms\n",
    "\n",
    "BOS = \"$\"  # le caractère indiquant le début d'un nom\n",
    "EOS = \"!\"  # le caractère indiquant la fin d'un nom\n",
    "\n",
    "def load_names(input_file):\n",
    "    with open(input_file, 'r') as names_file:\n",
    "        names = json.load(names_file)\n",
    "    origins = list(names.keys())\n",
    "    return origins, names\n",
    "\n",
    "def vocabulary():\n",
    "    voc = set()\n",
    "    for origin in all_origins:\n",
    "        for name in names_by_origin[origin]:\n",
    "            for letter in name:\n",
    "                voc.add(letter)\n",
    "    voc = list(voc)\n",
    "    voc.sort()\n",
    "    voc.append(BOS)\n",
    "    voc.append(EOS)\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_origins, names_by_origin = load_names(train_filename)\n",
    "all_letters = vocabulary()\n",
    "all_origins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformation des inputs du réseau\n",
    "\n",
    "On prépare une classe NameDataset qui hérite de la classe Dataset et qui est utilisée par un Dataloader de PyTorch pour gérer les données durant l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_originsfrom torch.utils.data import Dataset\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "\n",
    "    def __init__(self, names_by_origin, origin_list, vocabulary, bos_tag, eos_tag):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.origin_list = origin_list\n",
    "        self.tokenized_names = []\n",
    "        self.origin_index = []\n",
    "        self._generate_input_pairs(names_by_origin, bos_tag, eos_tag)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_names)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (torch.LongTensor([self.origin_index[item]]), torch.LongTensor(self.tokenized_names[item][:-1])), torch.LongTensor(self.tokenized_names[item][1:])\n",
    "\n",
    "    def _generate_input_pairs(self, names_by_origin, bos_tag, eos_tag):\n",
    "        bos_index = self.vocabulary.index(bos_tag)\n",
    "        eos_index = self.vocabulary.index(eos_tag)\n",
    "        for origin in names_by_origin:\n",
    "            for name in names_by_origin[origin]:\n",
    "                name_as_index_list = [bos_index] + [self.vocabulary.index(letter.lower()) for letter in name] + [eos_index]\n",
    "                self.tokenized_names.append(name_as_index_list)\n",
    "                self.origin_index.append(self.origin_list.index(origin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names_by_origin = json.load(open(test_filename,'r'))\n",
    "name_dataset = NameDataset(names_by_origin, all_origins, all_letters, BOS, EOS)\n",
    "test_name_dataset = NameDataset(test_names_by_origin, all_origins, all_letters, BOS, EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La fonction generate_one_hot_vector_table_for_classes crée un one-hot vector pour chaque origine d'un nom. Ainsi la table qui est généré contiendra 18 vecteurs, chacun ayant un 1 à la position correspondant à l'orgine, le reste état des 0.\n",
    "\n",
    "On utilise la même fonction pour convertir les lettres du vocabulaire en one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_one_hot_vector_table_for_classes(classes):\n",
    "    nb_class = len(classes)\n",
    "    one_hot_vectors = np.zeros((nb_class, nb_class))\n",
    "    for i in range(nb_class):\n",
    "        one_hot_vectors[i,i] = 1\n",
    "    return one_hot_vectors\n",
    "\n",
    "# On transforme les matrice numpy en tensor pour les insérer directement dans l'architecture neuronale\n",
    "letter_vectors = torch.FloatTensor(generate_one_hot_vector_table_for_classes(all_letters))\n",
    "origin_vectors = torch.FloatTensor(generate_one_hot_vector_table_for_classes(all_origins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création d'une architecture de réseau récurrent\n",
    "Le modèle GenerationRNN est un réseau récurrent qui prend en entrée la lettre précédente du nom à générer et la langue d'origine. Chacune est représentée par un one-hot vector. Donc, l'input du réseau est la concaténation des one-hot vectors de la langue d'origine et de la lettre. En sortie, le réseau estime un score pour chacune des lettre du vocabulaire.\n",
    "\n",
    "Dans cet exemple, les cellules de la couche caché du réseau récurrent sont des GRUs (voir le matériel du cours pour plus de détails).\n",
    "\n",
    "L'état cachée produite par le GRU est repris par un couche linéaire pour choisir la prochaine lettre à générer.\n",
    "\n",
    "Note: le terme embeddings est utilisé ici plutôt librement, car le réseau ne construit pas explicitement de plongements de caractères. Les one-hot vecteurs sont connectés directement à la couche cachée. Une couche cachée additionnelle de projection serait nécessaire pour obtenir des embeddings suite à l'entraînement du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerationRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_1, vector_2, hidden_state_size, nb_classes):\n",
    "        super().__init__()\n",
    "        self.class_embeddings = nn.Embedding.from_pretrained(vector_1)\n",
    "        self.token_sequence_embeddings = nn.Embedding.from_pretrained(vector_2)\n",
    "        joined_embedding_size = self.class_embeddings.embedding_dim + self.token_sequence_embeddings.embedding_dim\n",
    "        self.rnn = nn.GRU(joined_embedding_size, hidden_state_size, 2, bidirectional=False)\n",
    "        self.classification_layer = nn.Linear(hidden_state_size, nb_classes)\n",
    "\n",
    "    def forward(self, index_1, index_2, sequence_length):\n",
    "        joined_embedding = self._join_embedding(index_1, index_2, sequence_length)\n",
    "        rnn_output = self._handle_rnn_output(joined_embedding, sequence_length)\n",
    "        output = self.classification_layer(rnn_output)\n",
    "        return output, sequence_length\n",
    "\n",
    "    def _join_embedding(self, index_1, index_2, sequence_length):\n",
    "\n",
    "        # On a une origin pour chacun des noms de la batch\n",
    "        # Dimensions du tensor: 1 x origin_size x b \n",
    "        class_embedding = self.class_embeddings(index_1) \n",
    "        \n",
    "        # On a au maximum n_lettres pour tous les noms de la batch \n",
    "        # Dimensions du tensor: n_lettres x vocabulary_size x b\n",
    "        sequence_embeddings = self.token_sequence_embeddings(index_2) \n",
    "        \n",
    "        # on crée une \"tuile\" du vecteur de l'origine en le copiant pour chaque lettre du nom \n",
    "        # Dimensions du tensor: n_lettres x origine_size x b\n",
    "        max_sequence_length = torch.max(sequence_length)\n",
    "        tiled_class_embedding = class_embedding.repeat(1, max_sequence_length, 1)  \n",
    "        \n",
    "        # On colle la tuile au dessus des vecteurs de lettres du nom: \n",
    "        # Dimensions du tensor: n_lettres x (origin_size + vocabulary_size) x b\n",
    "        joined_embedding = torch.cat((tiled_class_embedding, sequence_embeddings), dim=2) \n",
    "\n",
    "        return joined_embedding\n",
    "\n",
    "    def _handle_rnn_output(self, x, x_lenghts):\n",
    "        # On \"pack\" les batch pour les envoyer dans le RNN\n",
    "        packed_batch = pack_padded_sequence(x, x_lenghts, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # On s'intéresse cette fois-ci aux output après chaque mot\n",
    "        rnn_output, _ = self.rnn(packed_batch)\n",
    "\n",
    "        # On \"repad\" les output pour les remettre dans une forme utilisable\n",
    "        unpacked_rnn_output, _ = pad_packed_sequence(rnn_output, batch_first=True)\n",
    "\n",
    "        return unpacked_rnn_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Création d'un métrique de perte (loss function)\n",
    "La métrique de perte est l'entropie croisée qui correspond ici à la classification correcte de la prochaine lettre sachant toutes les lettres précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenStateClassificationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, model_output, y_truth):\n",
    "        y_pred, sequence_length = model_output # y_pred : batch x name length x nb letters\n",
    "        loss = torch.FloatTensor([0])\n",
    "        # Pour chaque exemple d'entraînement\n",
    "        for example_index in range(y_pred.size()[0]):\n",
    "            example_length = sequence_length[example_index]\n",
    "            outputs_to_predict = y_pred[example_index, 0:example_length, :]\n",
    "            true_value = y_truth[example_index][0:example_length]\n",
    "            # On compare simultanément les prédictions de toutes les lettres du nom\n",
    "            loss += nn.functional.cross_entropy(outputs_to_predict, true_value)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement du modèle de réseau récurrent¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    x = [x for x, y in batch]\n",
    "    x_class = torch.stack([origin for origin, _ in x], dim=0)\n",
    "    x_sequence = [sequence for _, sequence in x]\n",
    "    x_true_length = [len(x) for x in x_sequence]\n",
    "    y = [y for x, y in batch]\n",
    "    return ((x_class, pad_sequence(x_sequence, batch_first=True), torch.LongTensor(x_true_length)), pad_sequence(y, batch_first=True))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(name_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "test_dataloader = DataLoader(test_name_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "set_seeds(42)\n",
    "\n",
    "model = GenerationRNN(origin_vectors, letter_vectors, 300, len(all_letters))\n",
    "\n",
    "experiment = Experiment('model/rnn_generation', model, loss_function=HiddenStateClassificationLoss(), optimizer=\"adam\")\n",
    "experiment.train(train_dataloader, test_dataloader, epochs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Génération de noms avec le modèle de réseau récurrent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input(origin, current_name):\n",
    "    origin_index = all_origins.index(origin)\n",
    "    letter_indexes = [all_letters.index(letter) for letter in current_name]\n",
    "    current_name_length = len(current_name)\n",
    "    return torch.LongTensor([origin_index]), torch.LongTensor(letter_indexes).unsqueeze(0), torch.LongTensor([current_name_length])\n",
    "\n",
    "def generate_name(language, starting_letter):\n",
    "    current_name = \"${}\".format(starting_letter.lower())\n",
    "    current_input = convert_to_input(language, current_name)\n",
    "    next_letter = None\n",
    "    while next_letter != EOS:\n",
    "        output = model(*current_input)[0][:,-1,:].detach().numpy()\n",
    "        best = np.argmax(output)\n",
    "        next_letter = all_letters[best]\n",
    "        current_name += next_letter\n",
    "        current_input = convert_to_input(language, current_name)\n",
    "    return current_name[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "def generate_names_for_each_origin(nb_generations):\n",
    "    for origin in all_origins:\n",
    "        generated_names = []\n",
    "        letters = random.sample(string.ascii_lowercase, nb_generations)\n",
    "        for letter in letters:\n",
    "            first_letter = letter.upper()\n",
    "            new_name = generate_name(origin, first_letter).capitalize()\n",
    "            generated_names.append(new_name)\n",
    "        print(origin, generated_names)\n",
    "\n",
    "generate_names_for_each_origin(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
